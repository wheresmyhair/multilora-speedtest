{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 04:44:58,089\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 04:45:03,584\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 04:45:04 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/yizhenjia/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-70B/snapshots/b4d08b7db49d488da3ac49adf25a6b9ac01ae338', speculative_config=None, tokenizer='/home/yizhenjia/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-70B/snapshots/b4d08b7db49d488da3ac49adf25a6b9ac01ae338', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/yizhenjia/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-70B/snapshots/b4d08b7db49d488da3ac49adf25a6b9ac01ae338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 04:45:14 utils.py:618] Found nccl from library libnccl.so.2\n",
      "INFO 06-03 04:45:14 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "\u001b[36m(RayWorkerWrapper pid=680509)\u001b[0m INFO 06-03 04:45:14 utils.py:618] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=680509)\u001b[0m INFO 06-03 04:45:14 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "WARNING 06-03 04:45:15 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=680509)\u001b[0m WARNING 06-03 04:45:15 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 06-03 04:45:26 model_runner.py:146] Loading model weights took 32.8599 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=680509)\u001b[0m INFO 06-03 04:45:31 model_runner.py:146] Loading model weights took 32.8599 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=680817)\u001b[0m INFO 06-03 04:45:14 utils.py:618] Found nccl from library libnccl.so.2\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=680817)\u001b[0m INFO 06-03 04:45:14 pynccl.py:65] vLLM is using nccl==2.20.5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=680817)\u001b[0m WARNING 06-03 04:45:15 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 06-03 04:45:38 distributed_gpu_executor.py:56] # GPU blocks: 4050, # CPU blocks: 3276\n",
      "INFO 06-03 04:45:46 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-03 04:45:46 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=680631)\u001b[0m INFO 06-03 04:45:48 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=680631)\u001b[0m INFO 06-03 04:45:48 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=680817)\u001b[0m INFO 06-03 04:45:31 model_runner.py:146] Loading model weights took 32.8599 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 06-03 04:46:26 model_runner.py:924] Graph capturing finished in 39 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=680509)\u001b[0m INFO 06-03 04:46:26 model_runner.py:924] Graph capturing finished in 38 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=\"/home/yizhenjia/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-70B/snapshots/b4d08b7db49d488da3ac49adf25a6b9ac01ae338\", \n",
    "    # enable_lora=True,\n",
    "    tensor_parallel_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,\n",
    "    max_tokens=256,\n",
    "    stop=[\"[/assistant]\"]\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",\n",
    "     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    sampling_params,\n",
    "    lora_request=LoRARequest(\"sql_adapter\", 1, '/vol/yizhenjia/projs/multilora-speedtest/lora/adapter_model_1')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
